{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core tools for working with storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from abc import ABC,abstractmethod\n",
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "import azure.storage.blob,azure.core.exceptions\n",
    "import boto3\n",
    "import shutil,re\n",
    "from typing import List,Tuple,Optional,Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *\n",
    "from configparser import SectionProxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_config(section_name:str=None,config_name:str='secrets/settings.ini'):\n",
    "    config_path=Path(config_name)\n",
    "    config=ConfigParser()\n",
    "    config.read(config_path)\n",
    "    if section_name is None:\n",
    "        return config\n",
    "    if section_name not in config:\n",
    "        raise Exception(f'Error: [{section_name}] section not found in {config_path}')\n",
    "    return dict(config.items(section_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(read_config(),ConfigParser)\n",
    "assert isinstance(read_config()['DEFAULT'],SectionProxy)\n",
    "assert isinstance(read_config('DEFAULT'),dict)\n",
    "assert read_config('local_cwd',config_name='test/settings.ini')['storage_type']=='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_dataset_archive_name(name:str) -> Optional[Tuple[str,...]]:\n",
    "    \"Returns (name,version) if `name` is a dataset archive name, `None` otherwise\"\n",
    "    match = re.match('^([\\./\\s\\w-]+)\\.(\\d+\\.\\d+\\.\\d+)\\.zip$',name)\n",
    "    return None if match is None else match.group(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(('dsetname', '0.0.1'), parse_dataset_archive_name('dsetname.0.0.1.zip'))\n",
    "test_eq(('dsetname.txt', '0.2.1'), parse_dataset_archive_name('dsetname.txt.0.2.1.zip'))\n",
    "test_eq(('path/to/dsetname', '0.0.1'), parse_dataset_archive_name('path/to/dsetname.0.0.1.zip'))\n",
    "test_eq(('//path/to/dsetname', '0.0.1'), parse_dataset_archive_name('//path/to/dsetname.0.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.0.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.a.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('.0.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('0.0.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.0.1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parse_dataset_archive_version(version:str) -> List[int]:\n",
    "    \"Returns (major,minor,patch) if `version` is a valid dataset archive version\"\n",
    "    match = re.match('^(\\d+)\\.(\\d+)\\.(\\d+)$',version)\n",
    "    if match is None: raise ValueError(f'Invalid version: {version}')\n",
    "    return [int(s) for s in match.group(1,2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([0,1,2],parse_dataset_archive_version('0.1.2'))\n",
    "test_eq([5,4,3],parse_dataset_archive_version('5.4.3'))\n",
    "test_fail(lambda: parse_dataset_archive_version('0.1.2.'))\n",
    "test_fail(lambda: parse_dataset_archive_version('0.1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def next_version(versions:List[str]=None,increment:str='patch'):\n",
    "    \"Return the version that should follow the last version in `versions`\"\n",
    "    v=[0,0,0] if versions is None else parse_dataset_archive_version(versions[-1])\n",
    "    if increment=='patch': v[2]+=1\n",
    "    elif increment=='minor': v[1]+=1;v[2]=0\n",
    "    elif increment=='major': v[0]+=1;v[1]=0;v[2]=0\n",
    "    else: raise ValueError(f'Unknown increment: {increment}')\n",
    "    return f'{v[0]}.{v[1]}.{v[2]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('0.0.1',next_version(None))\n",
    "test_eq('33.55.67',next_version(['2.4.60','33.55.66']))\n",
    "test_eq('0.1.0',next_version(None,'minor'))\n",
    "test_eq('1.0.0',next_version(None,'major'))\n",
    "test_eq('3.0.0',next_version(['2.4.60'],'major'))\n",
    "test_fail(lambda: next_version(None,'beta'))\n",
    "test_fail(lambda: next_version(['2.4.60','33.55.66a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def append_or_replace_verion(name,version):\n",
    "    parse_dataset_archive_version(version)\n",
    "    return '.'.join([name,version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_dataset_archive_folder(\n",
    "        path:str, name:str, versions:List[str]=None, version:str='patch') -> str:\n",
    "    \"Create a new dataset archive folder in `path`\"\n",
    "    src=Path(path)/name\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f'{src} not found')\n",
    "    if version in ['major','minor','patch']:\n",
    "        version=next_version(versions,version)\n",
    "    else:\n",
    "        parse_dataset_archive_version(version)\n",
    "        \n",
    "    archive_folder=Path(path)/'.'.join([name,version])\n",
    "    if archive_folder.exists(): \n",
    "        raise FileExistsError(f'Archive folder {archive_folder} exists')\n",
    "    if src.is_file(): \n",
    "        archive_folder.mkdir(parents=True)\n",
    "        shutil.copy(src,archive_folder)\n",
    "    else: \n",
    "        shutil.copytree(src,archive_folder)\n",
    "    # TODO: create/update manifest\n",
    "    # mf describes archive contents, data owner etc (TODO: manifest details TBC)\n",
    "    return f'{path}/{name}.{version}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def _rmtree(p):\n",
    "    try: shutil.rmtree(p)\n",
    "    except FileNotFoundError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_local_test_data():\n",
    "    test_files=['a/b/test_data.2.0.0.txt','test_data.txt']\n",
    "    for i in reversed(range(3)): test_files.insert(1,f'sub/test_data.0.0.{i}.txt')\n",
    "    for i,f in enumerate(test_files):\n",
    "        f='test/local_path/'+f\n",
    "        Path(f).parent.mkdir(parents=True,exist_ok=True)\n",
    "        with open(f, 'w') as _file: _file.write(f'a little bit of data {i}')\n",
    "    return test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ['test/local_path','test/storage_area']: _rmtree(p)\n",
    "_make_local_test_data()\n",
    "\n",
    "test_eq('test/local_path/test_data.txt.0.0.1',\n",
    "        make_dataset_archive_folder('test/local_path','test_data.txt'))\n",
    "test_eq('test/local_path/test_data.txt.2.5.0',\n",
    "        make_dataset_archive_folder('test/local_path','test_data.txt',['2.4.6'],'minor'))\n",
    "test_eq('test/local_path/sub.0.0.1',\n",
    "        make_dataset_archive_folder('test/local_path','sub'))\n",
    "# TODO: check archive folder contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StorageClientABC(ABC):\n",
    "    \"\"\"Defines functionality common to all storage clients\"\"\"\n",
    "    \n",
    "    def __init__(self, storage_name:str, config_name:str='secrets/settings.ini'):\n",
    "        \"Create a new storage client using the `storage_name` section of `config_name`\"\n",
    "        self.config=read_config(storage_name,config_name=config_name)\n",
    "\n",
    "    def _ls(self, p:Path, result:List[str], len_path_prefix:int=None):\n",
    "        if len_path_prefix is None: len_path_prefix=len(str(p).replace('\\\\','/'))\n",
    "        for _p in p.iterdir():\n",
    "            if _p.is_dir(): self._ls(_p,result,len_path_prefix)\n",
    "            else: result.append(str(_p).replace('\\\\','/')[len_path_prefix+1:])\n",
    "        \n",
    "    def ls(self, what:str='storage_area',name_starts_with:str=None) -> List[str]:\n",
    "        \"Return a list containing the names of files in either `storage_area` or `local_path`\"\n",
    "        result: List[str]=[]\n",
    "        p=Path(self.config[what])\n",
    "        p.mkdir(parents=True,exist_ok=True)\n",
    "        self._ls(p,result)\n",
    "        if name_starts_with is not None: \n",
    "            result=[r for r in result if r.startswith(name_starts_with)]\n",
    "        return sorted(result)\n",
    "        \n",
    "    @abstractmethod\n",
    "    def download(self, filename:str) -> Path: \n",
    "        \"Copy `filename` from `storage_area` to `local_path`\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def upload(self, filename:str, overwrite=False) -> Union[Path,str]: \n",
    "        \"Copy `filename` from `local_path` to `storage_area`\"\n",
    "        \n",
    "    def _sort_by_dataset_archive_version(self,version):\n",
    "        try: return tuple(parse_dataset_archive_version(version))\n",
    "        except: return (-1,-1,-1)\n",
    "        \n",
    "    def ls_versions(self, name:str, what:str='storage_area') -> Union[List[str],None]:\n",
    "        \"Return a list containing all versions of the specified archive `name`\"\n",
    "        files=[parse_dataset_archive_name(f) for f in self.ls(what)]\n",
    "        result=[f[1] for f in files if f is not None and f[0]==name]\n",
    "        if not result: return None\n",
    "        return sorted(result, key=self._sort_by_dataset_archive_version)\n",
    "        \n",
    "    def upload_dataset(self, name:str, version:str='patch') -> Union[Path,str]:\n",
    "        \"Create a new dataset archive and upload it to `storage_area`\"\n",
    "        archive_folder=make_dataset_archive_folder(\n",
    "                self.config['local_path'],name,self.ls_versions(name),version)\n",
    "        archive=shutil.make_archive(archive_folder,'zip',archive_folder)\n",
    "        return self.upload(f\"{archive_folder[len(self.config['local_path'])+1:]}.zip\")\n",
    "        \n",
    "    def download_dataset(self, name:str, version:str='latest', overwrite:bool=False) -> Path:\n",
    "        \"Download a dataset archive from `storage_area` and extract it to `local_path`\"\n",
    "        if version=='latest': \n",
    "            versions=self.ls_versions(name)\n",
    "            if versions is None:\n",
    "                raise ValueError('latest version requested but no versions exist in storage area')\n",
    "            version=versions[-1]\n",
    "        dst=Path(self.config['local_path'])/f'{name}.{version}'\n",
    "        if dst.exists() and not overwrite: return dst\n",
    "        archive=self.download(f'{name}.{version}.zip')\n",
    "        shutil.unpack_archive(str(archive),dst)\n",
    "        return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StorageClientABC.upload_dataset\" class=\"doc_header\"><code>StorageClientABC.upload_dataset</code><a href=\"__main__.py#L44\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StorageClientABC.upload_dataset</code>(**`name`**:`str`, **`version`**:`str`=*`'patch'`*)\n",
       "\n",
       "Create a new dataset archive and upload it to `storage_area`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StorageClientABC.upload_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `name`\n",
    "    - file or folder name\n",
    "    - which must exist in \"local_path\"\n",
    "    - without \"local_path\" prefix\n",
    "    - e.g. if\n",
    "        - \"local_path\" is \"~/storage_tools/test/local_path\"\n",
    "        - and you want to upload \"~/storage_tools/test/local_path/test_data.txt\" as a dataset\n",
    "        - you would pass the name \"test_data.txt\"\n",
    "- `version`\n",
    "    - \"major\", \"minor\" or \"patch\" to automatically create a new version or\n",
    "    - version literal that matches `\\d+\\.\\d+\\.\\d+` (e.g. \"1.0.45\")\n",
    "\n",
    "`upload_dataset` will;\n",
    "- create a folder `[local_path]/[name].[version]`\n",
    "    - if this folder already exists, as error will be raised\n",
    "- copy the file or folder contents (and all sub-folders) to this folder\n",
    "- create a manifest in this folder\n",
    "- create a zip archive, called `[name].[version].zip`, of this folder\n",
    "- upload the zip archive to remote storage\n",
    "\n",
    "Why no `overwrite` option?\n",
    "- It is not expected that archives will need to be overwritten\n",
    "    - as we want to be able to re-run old experiments using the data as it was\n",
    "- bad archives could be deleted via storage API (e.g. `storage_client.client.delete_blob('test.0.0.1.zip')`) or via storage bowsers\n",
    "    - we might want to add a soft delete, archive status etc to handle this kind of thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LocalStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses the local filesystem for both `storage_area` and `local_path`\"\"\"\n",
    "    \n",
    "    def _cp(self,from_key,to_key,filename,overwrite=False):\n",
    "        src=Path(self.config[from_key])/filename\n",
    "        dst=Path(self.config[to_key])/filename\n",
    "        if dst.exists() and not overwrite: \n",
    "            raise FileExistsError(f'{dst} exists and overwrite=False')\n",
    "        dst.parent.mkdir(parents=True,exist_ok=True)\n",
    "        shutil.copy(src,dst)\n",
    "        return dst\n",
    "        \n",
    "    def download(self,filename,overwrite=False):\n",
    "        try: self._cp('storage_area','local_path',filename,overwrite)\n",
    "        except FileExistsError: pass\n",
    "        return Path(self.config['local_path'])/filename\n",
    "        \n",
    "    def upload(self,filename,overwrite=False): \n",
    "        return self._cp('local_path','storage_area',filename,overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LocalStorageClient` will most often be used for local testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client=LocalStorageClient('local_test','test/settings.ini')\n",
    "assert storage_client.config['storage_type']=='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AzureStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses Azure for `storage_area` and the local filesystem `local_path`\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def client(self):\n",
    "        if not hasattr(self,'_client'):\n",
    "            service_client=azure.storage.blob.BlobServiceClient.from_connection_string(\n",
    "                self.config['conn_str'],self.config['credential'])\n",
    "            self._client=service_client.get_container_client(self.config['container'])\n",
    "        return self._client\n",
    "    \n",
    "    def ls(self,what='storage_area',name_starts_with=None):\n",
    "        if what=='local_path': return super().ls(what,name_starts_with)\n",
    "        result=[b.name for b in self.client.list_blobs(name_starts_with)]\n",
    "        return sorted(result)\n",
    "    \n",
    "    def download(self,filename,overwrite=False):\n",
    "        p=Path(self.config['local_path'])/filename\n",
    "        if p.exists() and not overwrite: return p\n",
    "        p.parent.mkdir(parents=True,exist_ok=True)\n",
    "        with open(p, 'wb') as f:\n",
    "            f.write(self.client.download_blob(filename).readall())\n",
    "        return p\n",
    "            \n",
    "    def upload(self,filename,overwrite=False): \n",
    "        p=Path(self.config['local_path'])/filename\n",
    "        try:\n",
    "            with open(p, 'rb') as f:\n",
    "                self.client.upload_blob(filename,f,overwrite=overwrite)\n",
    "            return f\"{self.config['storage_type']}:{self.config['container']}:{filename}\"\n",
    "        except azure.core.exceptions.ResourceExistsError as e:\n",
    "            raise FileExistsError(f'{e}\\noverwrite=False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "think about using ~/.aws/credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AwsStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses AWS for `storage_area` and the local filesystem `local_path`\"\"\"\n",
    "\n",
    "    @property\n",
    "    def client(self):\n",
    "        if not hasattr(self,'_client'):\n",
    "            self._client=boto3.client(service_name=self.config['service_name'],\n",
    "                                      aws_access_key_id=self.config['aws_access_key_id'],\n",
    "                                      aws_secret_access_key=self.config['aws_secret_access_key'])\n",
    "        return self._client\n",
    "    \n",
    "    def ls(self,what='storage_area',name_starts_with=None): \n",
    "        if what=='local_path': return super().ls(what,name_starts_with)\n",
    "        args=dict(Bucket=self.config['bucket'])\n",
    "        if name_starts_with is not None: args['Prefix']=name_starts_with\n",
    "        objects=self.client.list_objects_v2(**args)\n",
    "        if objects['KeyCount']==0: return []\n",
    "        result=[o['Key'] for o in objects['Contents'] if o['Size']>0]\n",
    "        return sorted(result)\n",
    "    \n",
    "    def download(self,filename): \n",
    "        p=Path(self.config['local_path'])/filename\n",
    "        if p.exists() and not overwrite: return p\n",
    "        p.parent.mkdir(parents=True,exist_ok=True)\n",
    "        self.client.download_file(\n",
    "                Filename='/'.join([self.config['local_path'],filename]),\n",
    "                Bucket=self.config['bucket'],\n",
    "                Key=filename)\n",
    "        return p\n",
    "        \n",
    "    def upload(self,filename,overwrite=False): \n",
    "        result=f\"{self.config['storage_type']}:{self.config['bucket']}:{filename}\"\n",
    "        if overwrite==False and filename in [self.ls(name_starts_with=filename)]:\n",
    "            raise FileExistsError(f'{result} exists and overwrite=False')\n",
    "        self.client.upload_file(\n",
    "                Filename='/'.join([self.config['local_path'],filename]),\n",
    "                Bucket=self.config['bucket'],\n",
    "                Key=filename)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would use the same property keys in settings.ini as the boto3 API but `ConfigParser` converts keys to lower case by default.\n",
    "\n",
    "So if boto3 has a parameter called `Bucket`, we need `bucket=a-bucket-name` settings.ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "If we pass `None` to `Prefix` when listing objects (e.g. `list_objects_v2(Prefix=None)`) AWS raises an error.\n",
    "This is why we have to create and unpack the `args` dictionary when we call `list_objects_v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def new_storage_client(storage_name,config_name='secrets/settings.ini'):\n",
    "    \"Returns a storage client based on the configured `storage_type`\"\n",
    "    config=read_config(storage_name,config_name=config_name)\n",
    "    storage_type=config['storage_type']\n",
    "    if storage_type=='local': return LocalStorageClient(storage_name, config_name)\n",
    "    elif storage_type=='azure': return AzureStorageClient(storage_name, config_name)\n",
    "    elif storage_type=='aws': return AwsStorageClient(storage_name, config_name)\n",
    "    else: raise ValueError(f'Unknown storage_type: {storage_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(lambda: new_storage_client('gcp_dummy','test/settings.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ['test/local_path','test/storage_area']: _rmtree(p)\n",
    "    \n",
    "storage_client=new_storage_client('local_test','test/settings.ini')\n",
    "assert isinstance(storage_client,LocalStorageClient)\n",
    "assert storage_client.config['storage_type']=='local'\n",
    "test_eq([],storage_client.ls())\n",
    "test_eq([],storage_client.ls('local_path'))\n",
    "    \n",
    "test_files=_make_local_test_data()\n",
    "test_eq([],storage_client.ls())\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "test_eq(['a/b/test_data.2.0.0.txt'],storage_client.ls('local_path','a/b'))\n",
    "test_eq([],storage_client.ls('local_path','does/not/exist'))\n",
    "        \n",
    "for f in test_files: storage_client.upload(f)\n",
    "test_eq(test_files,storage_client.ls())\n",
    "test_eq(['a/b/test_data.2.0.0.txt'],storage_client.ls(name_starts_with='a/b'))\n",
    "test_eq([],storage_client.ls(name_starts_with='does_no_exist'))\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "_rmtree('test/local_path')\n",
    "test_eq([],storage_client.ls('local_path'))\n",
    "\n",
    "for f in test_files: storage_client.download(f)\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "test_eq('a little bit of data 4',open('test/local_path/test_data.txt').read())\n",
    "\n",
    "with open('test/local_path/test_data.txt', 'w') as _file: _file.write('upd')\n",
    "test_eq('upd',open('test/local_path/test_data.txt').read())\n",
    "storage_client.download('test_data.txt')\n",
    "test_eq('upd',open('test/local_path/test_data.txt').read())\n",
    "storage_client.download('test_data.txt',True)\n",
    "test_eq('a little bit of data 4',open('test/local_path/test_data.txt').read())\n",
    "\n",
    "test_fail(lambda: storage_client.upload('test_data.txt'))\n",
    "storage_client.upload('test_data.txt',True)\n",
    "\n",
    "test_eq(None,storage_client.ls_versions('this/does/not/exitst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _t(expected,upload_name,version='patch'):\n",
    "    test_eq(Path(f'test/storage_area/{expected}'),storage_client.upload_dataset(upload_name,version))\n",
    "_t('test_data.txt.0.0.1.zip','test_data.txt')\n",
    "_t('sub.0.0.1.zip','sub')\n",
    "_t('sub/test_data.0.0.2.txt.0.0.1.zip','sub/test_data.0.0.2.txt')\n",
    "_t('a.3.0.0.zip','a','3.0.0')\n",
    "_t('a.3.0.1.zip','a')\n",
    "# TODO: check zip contents\n",
    "_rmtree('test/local_path/a.3.0.0')\n",
    "_rmtree('test/local_path/a.3.0.1')\n",
    "test_eq(Path('test/local_path/a.3.0.1'),storage_client.download_dataset('a'))\n",
    "test_eq(Path('test/local_path/a.3.0.0'),storage_client.download_dataset('a','3.0.0'))\n",
    "# TODO: check folder contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client=new_storage_client('azure_dummy','test/settings.ini')\n",
    "assert isinstance(storage_client,AzureStorageClient)\n",
    "storage_client=new_storage_client('aws_dummy','test/settings.ini')\n",
    "assert isinstance(storage_client,AwsStorageClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up test data\n",
    "for p in ['test/local_path','test/storage_area']: _rmtree(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
