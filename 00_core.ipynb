{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core tools for working with storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from abc import ABC,abstractmethod\n",
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "import azure.storage.blob,azure.core.exceptions\n",
    "import boto3\n",
    "import shutil,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *\n",
    "from configparser import SectionProxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_config(section_name=None,config_name='secrets/settings.ini'):\n",
    "    config_path=Path(config_name)\n",
    "    config=ConfigParser()\n",
    "    config.read(config_path)\n",
    "    if section_name is None:\n",
    "        return config\n",
    "    if section_name not in config:\n",
    "        raise Exception(f'Error: [{section_name}] section not found in {config_path}')\n",
    "    return dict(config.items(section_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(read_config(),ConfigParser)\n",
    "assert isinstance(read_config()['DEFAULT'],SectionProxy)\n",
    "assert isinstance(read_config('DEFAULT'),dict)\n",
    "assert read_config('local_cwd',config_name='test/settings.ini')['storage_type']=='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_dataset_archive_name(name):\n",
    "    \"Returns (name,version) if `name` is a dataset archive name, `None` otherwise\"\n",
    "    match = re.match('^([\\./\\s\\w-]+)\\.(\\d+\\.\\d+\\.\\d+)\\.zip$',name)\n",
    "    if match is None: return None\n",
    "    return match.group(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(('dsetname', '0.0.1'), parse_dataset_archive_name('dsetname.0.0.1.zip'))\n",
    "test_eq(('dsetname.txt', '0.2.1'), parse_dataset_archive_name('dsetname.txt.0.2.1.zip'))\n",
    "test_eq(('path/to/dsetname', '0.0.1'), parse_dataset_archive_name('path/to/dsetname.0.0.1.zip'))\n",
    "test_eq(('//path/to/dsetname', '0.0.1'), parse_dataset_archive_name('//path/to/dsetname.0.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.0.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.a.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('.0.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('0.0.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.0.1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parse_dataset_archive_version(version):\n",
    "    match = re.match('^(\\d+)\\.(\\d+)\\.(\\d+)$',version)\n",
    "    if match is None: return None\n",
    "    return [int(s) for s in match.group(1,2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq((0, 1, 2),parse_dataset_archive_version('0.1.2'))\n",
    "test_eq((5, 4, 3),parse_dataset_archive_version('5.4.3'))\n",
    "test_eq(None,parse_dataset_archive_version('0.1.2.'))\n",
    "test_eq(None,parse_dataset_archive_version('0.1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def next_version(versions,increment='patch'):\n",
    "    v=[0,0,0] if versions is None else parse_dataset_archive_version(versions[-1])\n",
    "    if increment=='patch': v[2]+=1\n",
    "    elif increment=='minor': v[1]+=1\n",
    "    elif increment=='major': v[0]+=1\n",
    "    else: raise ValueError(f'Unknown increment: {increment}')\n",
    "    return f'{v[0]}.{v[1]}.{v[2]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('0.0.1',next_version(None))\n",
    "test_eq('33.55.67',next_version(['2.4.60','33.55.66']))\n",
    "test_eq('0.1.0',next_version(None,'minor'))\n",
    "test_eq('1.0.0',next_version(None,'major'))\n",
    "test_eq('3.4.60',next_version(['2.4.60'],'major'))\n",
    "test_fail(lambda: next_version(None,'beta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_dataset_archive_folder(path,versions,name,version='patch'):\n",
    "    \"Create a new dataset archive folder in `local_path`\"\n",
    "    src=Path(path)/name\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f'{src} not found')\n",
    "    if version in ['major','minor','patch']:\n",
    "        version=next_version(versions,version)\n",
    "    elif parse_dataset_archive_version(version) is None:\n",
    "        raise ValueError(f'Invalid version: {version}')\n",
    "    archive_folder=Path(path)/'.'.join([name,version])\n",
    "    if archive_folder.exists(): \n",
    "        raise FileExistsError(f'Archive folder {archive_folder} exists')\n",
    "    if src.is_file(): \n",
    "        archive_folder.mkdir(parents=True)\n",
    "        shutil.copy(src,archive_folder)\n",
    "    else: \n",
    "        shutil.copytree(src,archive_folder)\n",
    "    # TODO: create/update manifest\n",
    "    return archive_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rmtree(p):\n",
    "    try: shutil.rmtree(p)\n",
    "    except FileNotFoundError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_local_test_data():\n",
    "    test_files=['a/b/test_data.2.0.0.txt','test_data.txt']\n",
    "    for i in reversed(range(3)): test_files.insert(1,f'sub/test_data.0.0.{i}.txt')\n",
    "    for i,f in enumerate(test_files):\n",
    "        f='test/local_path/'+f\n",
    "        Path(f).parent.mkdir(parents=True,exist_ok=True)\n",
    "        with open(f, 'w') as _file: _file.write(f'a little bit of data {i}')\n",
    "    return test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ['test/local_path','test/storage_area']: _rmtree(p)\n",
    "_make_local_test_data()\n",
    "\n",
    "test_eq(Path('test/local_path/test_data.txt.0.0.1'),\n",
    "        make_dataset_archive_folder('test/local_path',None,'test_data.txt'))\n",
    "test_eq(Path('test/local_path/test_data.txt.2.5.6'),\n",
    "        make_dataset_archive_folder('test/local_path',['2.4.6'],'test_data.txt','minor'))\n",
    "test_eq(Path('test/local_path/sub.0.0.1'),\n",
    "        make_dataset_archive_folder('test/local_path',None,'sub'))\n",
    "# TODO: check archive folder contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StorageClientABC(ABC):\n",
    "    \"\"\"Defines functionality common to all storage clients\"\"\"\n",
    "    \n",
    "    def __init__(self,storage_name,config_name='secrets/settings.ini'):\n",
    "        \"Create a new storage client using the `storage_name` section of `config_name`\"\n",
    "        self.config=read_config(storage_name,config_name=config_name)\n",
    "\n",
    "    def _ls(self,p,result,len_path_prefix=None):\n",
    "        if len_path_prefix is None: len_path_prefix=len(str(p).replace('\\\\','/'))\n",
    "        for _p in p.iterdir():\n",
    "            if _p.is_dir(): self._ls(_p,result,len_path_prefix)\n",
    "            else: result.append(str(_p).replace('\\\\','/')[len_path_prefix+1:])\n",
    "        \n",
    "    def ls(self,what='storage_area'):\n",
    "        \"Return a list containing the names of files in either `storage_area` or `local_path`\"\n",
    "        result,p=[],Path(self.config[what])\n",
    "        p.mkdir(parents=True,exist_ok=True)\n",
    "        self._ls(p,result)\n",
    "        sorted(result)\n",
    "        return result\n",
    "        \n",
    "    @abstractmethod\n",
    "    def download(self,filename): \n",
    "        \"Copy `filename` from `storage_area` to `local_path`\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def upload(self,filename,overwrite=False): \n",
    "        \"Copy `filename` from `local_path` to `storage_area`\"\n",
    "        \n",
    "    def ls_versions(self,name,what='storage_area'):\n",
    "        \"Return a list containing all versions of the specified archive `name`\"\n",
    "        files=[parse_dataset_archive_name(f) for f in self.ls(what)]\n",
    "        result=[f[1] for f in files if f is not None and f[0]==name]\n",
    "        if not result: return None\n",
    "        return sorted(result, key=lambda v: parse_dataset_archive_version(v))\n",
    "        \n",
    "    def upload_dataset(self,name,version='patch'):\n",
    "        \"Create a new dataset archive and upload it to `storage_area`\"\n",
    "        archive_folder=make_dataset_archive_folder(\n",
    "                self.config['local_path'],self.ls_versions(name),name,version)\n",
    "        archive=shutil.make_archive(archive_folder,'zip',archive_folder)\n",
    "        return self.upload(Path(archive).name)\n",
    "        \n",
    "    def download_dataset(self,name,version='latest'):\n",
    "        \"Download a dataset archive from `storage_area` and extract it to `local_path`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(StorageClientABC.upload_archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`upload_archive`\n",
    "\n",
    "- `name`\n",
    "    - file or folder name\n",
    "- `version`\n",
    "    - \"major\", \"minor\" or \"patch\" to automatically create a new version or\n",
    "    - version literal `\\d+\\.\\d+\\.\\d+` (e.g. \"1.0.45\")\n",
    "\n",
    "The name of the new archive will be `[folder name|file name without format-specific extension][version].zip` and will contain\n",
    "- the specified file or all files in the specified folder (and all sub-folders)\n",
    "- a manifest describing archive contents, data owner etc (TODO: manifest details TBC)\n",
    "\n",
    "If a folder called `[local_path][name][version]` already exists, we will\n",
    "- create a manifest in this folder (if it doesn't already exist)\n",
    "- archive and upload this folder\n",
    "\n",
    "Otherwise, we will \n",
    "- create a folder called `[local_path][name][version]`\n",
    "- copy the file or folder contents to `[local_path][name][version]`\n",
    "- create a manifest in this folder\n",
    "- archive and upload this folder\n",
    "\n",
    "Why no `overwrite` option?\n",
    "- It is not expected that archives will need to be overwritten\n",
    "    - as we want to be able to re-run old experiments using the data as it was\n",
    "- bad archives could be deleted via storage API (e.g. `storage_client.client.delete_blob('test.0.0.1.zip')`) or via storage bowsers\n",
    "    - we might want to add a soft delete, archive status etc to handle this kind of thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LocalStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses the local filesystem for both `storage_area` and `local_path`\"\"\"\n",
    "    \n",
    "    def _cp(self,from_key,to_key,filename,overwrite=False):\n",
    "        src=Path(self.config[from_key])/filename\n",
    "        dst=Path(self.config[to_key])/filename\n",
    "        if dst.exists() and not overwrite: \n",
    "            raise FileExistsError(f'{dst} exists and overwrite=False')\n",
    "        dst.parent.mkdir(parents=True,exist_ok=True)\n",
    "        shutil.copy(src,dst)\n",
    "        return dst\n",
    "        \n",
    "    def download(self,filename,overwrite=False):\n",
    "        try: self._cp('storage_area','local_path',filename,overwrite)\n",
    "        except FileExistsError: pass\n",
    "        \n",
    "    def upload(self,filename,overwrite=False): \n",
    "        return self._cp('local_path','storage_area',filename,overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LocalStorageClient` will most often be used for local testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client=LocalStorageClient('local_test','test/settings.ini')\n",
    "assert storage_client.config['storage_type']=='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AzureStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses Azure for `storage_area` and the local filesystem `local_path`\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def client(self):\n",
    "        if not hasattr(self,'_client'):\n",
    "            service_client=azure.storage.blob.BlobServiceClient.from_connection_string(\n",
    "                self.config['conn_str'],self.config['credential'])\n",
    "            self._client=service_client.get_container_client(self.config['container'])\n",
    "        return self._client\n",
    "    \n",
    "    def ls(self,what='storage_area'):\n",
    "        if what=='local_path': return super().ls(what)\n",
    "        result=[b.name for b in self.client.list_blobs()]\n",
    "        sorted(result)\n",
    "        return result\n",
    "    \n",
    "    def download(self,filename,overwrite=False):\n",
    "        p=Path(self.config['local_path'])/filename\n",
    "        if p.exists() and not overwrite: return\n",
    "        p.parent.mkdir(parents=True,exist_ok=True)\n",
    "        with open(p, 'wb') as f:\n",
    "            f.write(self.client.download_blob(filename).readall())\n",
    "            \n",
    "    def upload(self,filename,overwrite=False): \n",
    "        p=Path(self.config['local_path'])/filename\n",
    "        try:\n",
    "            with open(p, 'rb') as f:\n",
    "                self.client.upload_blob(filename,f,overwrite=overwrite)\n",
    "            return f\"{config['storage_type']}:{config['container']}:{filename}\"\n",
    "        except azure.core.exceptions.ResourceExistsError as e:\n",
    "            raise FileExistsError(f'{e}\\noverwrite=False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AwsStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses AWS for `storage_area` and the local filesystem `local_path`\"\"\"\n",
    "    def ls(self,what='storage_area'): pass \n",
    "    def download(self,filename): pass \n",
    "    def upload(self,filename,overwrite=False): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def new_storage_client(storage_name,config_name='secrets/settings.ini'):\n",
    "    \"Returns a storage client based on the configured `storage_type`\"\n",
    "    config=read_config(storage_name,config_name=config_name)\n",
    "    storage_type=config['storage_type']\n",
    "    if storage_type=='local': return LocalStorageClient(storage_name, config_name)\n",
    "    elif storage_type=='azure': return AzureStorageClient(storage_name, config_name)\n",
    "    elif storage_type=='aws': return AwsStorageClient(storage_name, config_name)\n",
    "    else: raise ValueError(f'Unknown storage_type: {storage_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(lambda: new_storage_client('gcp_dummy','test/settings.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ['test/local_path','test/storage_area']: _rmtree(p)\n",
    "    \n",
    "storage_client=new_storage_client('local_test','test/settings.ini')\n",
    "assert isinstance(storage_client,LocalStorageClient)\n",
    "assert storage_client.config['storage_type']=='local'\n",
    "test_eq([],storage_client.ls())\n",
    "test_eq([],storage_client.ls('local_path'))\n",
    "    \n",
    "test_files=_make_local_test_data()\n",
    "test_eq([],storage_client.ls())\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "        \n",
    "for f in test_files: storage_client.upload(f)\n",
    "test_eq(test_files,storage_client.ls())\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "_rmtree('test/local_path')\n",
    "test_eq([],storage_client.ls('local_path'))\n",
    "\n",
    "for f in test_files: storage_client.download(f)\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "test_eq('a little bit of data 4',open('test/local_path/test_data.txt').read())\n",
    "\n",
    "with open('test/local_path/test_data.txt', 'w') as _file: _file.write('upd')\n",
    "test_eq('upd',open('test/local_path/test_data.txt').read())\n",
    "storage_client.download('test_data.txt')\n",
    "test_eq('upd',open('test/local_path/test_data.txt').read())\n",
    "storage_client.download('test_data.txt',True)\n",
    "test_eq('a little bit of data 4',open('test/local_path/test_data.txt').read())\n",
    "\n",
    "test_fail(lambda: storage_client.upload('test_data.txt'))\n",
    "storage_client.upload('test_data.txt',True)\n",
    "\n",
    "test_eq(None,storage_client.ls_versions('this/does/not/exitst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(Path('test/storage_area/test_data.txt.0.0.1.zip'),storage_client.upload_dataset('test_data.txt'))\n",
    "test_eq(Path('test/storage_area/sub.0.0.1.zip'),storage_client.upload_dataset('sub'))\n",
    "test_eq(Path('test/storage_area/a.1.0.0.zip'),storage_client.upload_dataset('a','1.0.0'))\n",
    "test_eq(Path('test/storage_area/a.1.0.1.zip'),storage_client.upload_dataset('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client=new_storage_client('azure_dummy','test/settings.ini')\n",
    "assert isinstance(storage_client,AzureStorageClient)\n",
    "storage_client=new_storage_client('aws_dummy','test/settings.ini')\n",
    "assert isinstance(storage_client,AwsStorageClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up test data\n",
    "for p in ['test/local_path','test/storage_area']: _rmtree(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
